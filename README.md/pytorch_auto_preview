2026.02.12
📚 PyTorch 학습 2일차 정리

──────────────────────────────
1️⃣ view 사용 (텐서 모양 변경)

✔ view란?
- 텐서의 데이터는 그대로 유지하면서 shape(모양)만 변경하는 함수
- 전체 원소 개수는 반드시 동일해야 함

✔ 예시

x = torch.randn(2, 3, 4)

# 전체 원소 개수
2 * 3 * 4 = 24

y = x.view(2, -1)

- 첫 번째 값 2는 그대로 유지
- 전체 원소가 24이므로
  2 * ? = 24
  ? = 12

따라서
y.shape → [2, 12]

✔ 핵심 정리
- view는 "재배열"이지 데이터 변경이 아님
- 전체 원소 수는 항상 같아야 함
- -1은 자동 계산

──────────────────────────────
2️⃣ 미분 복습

✔ 예시: f(x) = 3x + 1

미분 과정:

3x + 1
= 3 * x¹

→ 3 * 1 * x⁰
→ 3 * 1 * 1
= 3

따라서
f'(x) = 3

✔ Gradient 초기화

optimizer를 사용하지 않을 경우

x.grad.zero_()

를 사용해서 gradient 누적을 초기화해야 함

※ gradient는 기본적으로 계속 누적(accumulate)됨

──────────────────────────────
3️⃣ nn.Linear 정리

✔ weight shape
(출력값 개수, 입력값 개수)

✔ bias shape
(출력값 개수, )

✔ 기본 수식 형태 (고정)

y = w1x1 + w2x2 + ... + b

→ 수식 구조는 고정
→ 학습 과정에서 바뀌는 것은 weight와 bias 값

──────────────────────────────
4️⃣ torch.linspace

✔ 의미
원하는 시작점과 끝점 사이를
동일한 간격으로 나눈 벡터 생성

✔ 예시

torch.linspace(0, 10, steps=5)

→ 0부터 10까지
→ 동일 간격으로
→ 5개 생성

──────────────────────────────
5️⃣ Gradient의 의미

✔ Gradient란?

"이 방향으로 가면 loss가 증가한다"는 의미

✔ 따라서 학습은

Gradient의 반대 방향으로 이동

(= Loss가 감소하는 방향)

이를 Gradient Descent라고 함

──────────────────────────────

💡 오늘의 핵심 한 줄 정리

Gradient는 loss가 증가하는 방향을 알려주고,
우리는 그 반대 방향으로 이동하며
weight를 업데이트한다.
