<Colibration> :
모델의 예측한 확신도가 실제 성능과 얼마나 일치하는지 
- Accuracy vs Calibration :
 Accuracy(정확도) : 전체적으로 얼마나 맞추었는가?
 Calibration : 확신도 표현이 얼마나 정확한가?
- Overconfident(과신) :
   모델이 실제보다 더 확신에 찬 예측을 한 것

<Reliability(신뢰성)>
: 모델이 다양한 데이터셋이나 실세계 환경에서 시간경과에 따라 얼마나 일관성 있고,
정확하며, 안전하고, 예측가능한 결과를 생성하는지를 나타내는 능력
단순하게 정확도(Acuuracy)를 넘어, 실제 배포 환경에서도 모델이 신뢰할 수 있게 
작동함을 보장하는 개념

<Reliability Diagram>
: 예측 확률과 실제 정답 비율을 시각적으로 비교하는 도구 
  이상적인 모델은 예측 확률과 실제 비율이 일치하는 신뢰도 곡선을 보임
  Colibration 이 잘 될 수록 직선에 가깝게 나옴
  
  표현방법 : 
    - 확률을 구 간으로 나눔
    - 각 구간마다 평균 예측 확률, 실제 정답 비율을 비교해서 그래프로 그림
    - 직선 아래쪽 : 예측이 과함 (예측값 > 관측값)
    - 직선 위쪽 : 예측이 약함(예측값 < 관측값)

<ECE(기대 보정 오차)>
: 예측 확률의 구간 별 가중 평균 오차
ECE가 0에 가까울수록 완벽한  calibration

<Reliability Diagram 및 ECE를 통한 모델의 확률 켈리브레이션을 평가>
x축(Confidence) : 모델의 예측이 맞을 확률
y축(Empirical Accuracy) : 실제로 해당 Confidence 구간에서 정답이었던 비율
막대: 각 confidence 구간에서의 실제 정확도 (empirical accuracy)
점: 해당 구간의 평균 예측 확률 (mean confidence)
점이 점선 아래 : 모델이 자신감 과대(오버컨피던트)
점이 점선 위 : 모델이 자신감 과대(언더컨피던트)
<그래프 결과> :
본 모델은 Validation 데이터에서 ECE 0.0065를 기록하며, 
예측 확률과 실제 정답률 간의 차이가 거의 없는 우수한 확률적 calibration 특성을 보임.

<Temperature Scaling> :
모델 weight(구조)는 그대로 두고 Validation set 에서 “확률이 가장 잘 맞도록” 만드는 
T를 찾는 것
  - T = 1 : 기본 소프트맥스 함수와 동일
  - T > 1 :
    확률 분포가 부드러워지고(flat), 가장 높은 확률값과 낮은 확률 값 사이의 차이가 줄어듬
  - T < 1 : 
    확률 분포가 뾰족해지고(peak), 가장 
  - 1이상의 값으로 로직을 나누는 경우 :
    다양성을 증가시키고 더많은 실수를 유발함
출력 확률을 부드럽게 조정하는 방법(덜 자신있는 방향으로)


<Temperature Scaling을 통한 Calibration 개선>
본 모델은 Validation 데이터에서 ECE 0.00887을 기록함.
Temperatur Scaling 을 통해 최적 Temperature : : 1.119456648826599 을 발견하였으며,
Temperature Scaling을 적용한 결과 ECE는 0.00680으로 감소함.
이는 약 23%의 calibration 오차 감소에 해당하며,
모델의 예측 확률이 실제 정답 확률에 더 근접하도록 보정되었음을 의미함.
해당 실험을 통해 단순 정확도뿐 아니라,
모델의 확률적 신뢰도까지 체계적으로 개선 및 검증함.

